# README â€” Advanced Evaluation Prompts for LMMs

This repository contains two high-precision prompts designed for **evaluating large language models (LLMs/LMMs)** in structured, reproducible ways. These prompts are intended to support model comparison, reasoning analysis, and release-readiness evaluation for technical documents.

The prompts themselves are stored in the separate repository:  
**`LMM-Prompts/Prompts/`**

---

## ðŸ“˜ Included Prompts

### **1. Structured Logical Argument Test Prompt**  
**File:** [`Structured_Logical_Argument_Test_Prompt.md`](https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Structured_Logical_Argument_Test_Prompt.md)

A comprehensive suite for evaluating an LLMâ€™s **logical reasoning, argument analysis, ambiguity handling, constraint-following, ethical reasoning, creativity, quantitative skills, and meta-reflection**.

It includes:

- The **8-Step Logical Rubric** (validity, soundness, assumptions, reasoning method)
- 26 sub-prompts (Aâ€“AC) covering:
  - Scientific & philosophical argument analysis
  - Logical fallacy detection
  - Ambiguous language interpretation
  - Humor, cultural nuance, and tone shifting
  - Instruction-following under constraints
  - Ethical dilemmas
  - Creative problem-solving and divergent thinking
  - Quantitative reasoning
  - Model self-reflection and personality profiling

---

### **2. Technical Report / White Paper Public Release Review Prompt**  
**File:** [`Technical_Report_White_Paper_Pre_Publication_Test_Prompt.md`](https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Technical_Report_White_Paper_Pre_Publication_Test_Prompt.md)

A structured evaluation prompt for determining whether a **technical report or white paper** is ready for public release on GitHub, arXiv, SSRN, or a corporate site.

It produces a fully structured **Release Readiness Assessment**, reviewing:

- Clarity, honesty, and framing of claims  
- Practical usefulness for practitioners/researchers  
- Transparency & reproducibility  
- Safety & ethics concerns  
- Technical completeness  
- Priority action items and quick fixes  

Ideal for anyone releasing technical research, model studies, reports, or public documentation.

---

## ðŸŽ¯ Purpose of This Repository

This repo provides two targeted, high-value evaluation tools:

1. **A deep-dive reasoning and logic assessment prompt**  
2. **A professional-grade release readiness review prompt**

These are useful for:

- AI researchers  
- Technical writers  
- Model evaluators  
- Prompt engineers  
- Practitioners preparing public reports  

---

## ðŸ§  How to Use the Prompts

1. Open the prompt file via the links above.  
2. Copy the entire prompt into your LLM of choice.  
3. Provide the argument or document when instructed.  
4. Review the structured output the LLM generates.  
5. Optionally compare results across multiple LMMs.

---

## ðŸ“„ License

This project is licensed under the terms of the **Creative Commons Attribution 4.0 International (CC BY 4.0)** license.

You are free to:

- **Share** â€” copy and redistribute  
- **Adapt** â€” remix and build upon  

As long as you provide proper attribution.

License File (in this repo):  
[`LICENSE`](LICENSE)

---

## ðŸ“‚ File Links

- [`LMM-Prompts/Prompts/Structured_Logical_Argument_Test_Prompt.md`](https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Structured_Logical_Argument_Test_Prompt.md)  
- [`LMM-Prompts/Prompts/Technical_Report_White_Paper_Pre_Publication_Test_Prompt.md`](https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Technical_Report_White_Paper_Pre_Publication_Test_Prompt.md)

---

If you want, I can also generate:

- A GitHub Pages landing page  
- A repo banner image  
- A compact summary for Reddit or documentation sites  
- Version numbers and changelog entries  

Just let me know!
