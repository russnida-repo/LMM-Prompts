# README â€” Argument Analysis, Technical Review, and Academic Publicationâ€‘Readiness Prompts

This repository contains **three highâ€‘precision evaluation prompts** designed to support:

- **Structured argument analysis**
- **Technical report / white paper release assessment**
- **Academic paper publicationâ€‘readiness evaluation**

These prompts help users evaluate reasoning quality, documentation clarity, and scientific rigor using **clear, repeatable frameworks**.

All prompt files are stored in:

**`LMM-Prompts/Prompts/`**

---

# ðŸ“˜ Included Prompts

---

## **1. Structured Logical Argument Test Prompt**
**File:**  
https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Structured_Logical_Argument_Test_Prompt.md

A complete, multiâ€‘stage framework for analyzing the strength and structure of **any argument**.

### **What It Does**
- Extracts explicit premises & hidden assumptions  
- Rewrites the argument in formal logical structure  
- Tests:
  - **Validity** â€” does the conclusion logically follow?  
  - **Soundness** â€” are the premises defensible or true?  
- Clarifies ambiguous terms and reasoning methods  
- Stressâ€‘tests the argument using:
  - Counterarguments  
  - Alternative interpretations  
  - Changed assumptions  
  - Generalization tests  
- Detects and explains logical fallacies  
- Steelmans the argument into a stronger, clearer version  
- Produces a labeled, formal proof of the improved argument  
- Provides a summary evaluation and final **Pass / Partial / Fail** verdict  

### **Best For**
- Philosophical arguments  
- Historical claims  
- Public policy debates  
- Online discussions  
- Opinion pieces and persuasive writing  
- Any reasoning that needs formal evaluation  

---

## **2. Technical Report / White Paper Public Release Review Prompt**
**File:**  
https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Technical_Report_White_Paper_Pre_Publication_Test_Prompt.md

A structured checklistâ€‘driven evaluation for determining whether a **technical report or white paper** is ready for public release.

### **What It Does**
Guides the model through a complete **Release Readiness Assessment**, including:

- Critical issues that block public release  
- Claim framing and scope evaluation  
- Transparency and reproducibility review  
- Practical usefulness for end users  
- Safety, ethics, and risk considerations  
- Technical completeness (figures, tables, references, appendices)  
- Quick fixes and enhancement recommendations  
- Priority action items  

### **Best For**
- Technical reports  
- Engineering documentation  
- Scientific white papers  
- Research summaries  
- GitHub documentation  
- arXiv / SSRN preprints  

---

## **3. Academic Paper Publicationâ€‘Readiness Evaluation Prompt**
**File:**  
https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Academic%20Paper%20Review%20Prompt.md

A structured evaluation prompt for determining whether an **academic research paper is ready for submission** to a conference or journal.

This prompt does **not** generate a formal peerâ€‘review report.  
Instead, it evaluates publication readiness by identifying scientific, methodological, structural, and presentation issues that could block acceptance.

### **What It Does**
- Scans contributions, structure, and clarity  
- Checks whether each major claim is supported by inâ€‘paper evidence  
- Evaluates methodology (controls, baselines, reproducibility, statistics)  
- Reviews presentation quality (figures, tables, notation, writing)  
- Flags missing, ambiguous, or unverifiable information  
- Produces:
  - Major issues blocking submission  
  - Minor issues  
  - Questions for authors  
  - Publicationâ€‘readiness recommendation  
  - Expected revision scope  
  - Verification & confidence assessment  

### **Best For**
- Authors preparing papers for submission  
- Research teams conducting internal checks  
- Advisors reviewing student manuscripts  
- Labs preparing earlyâ€‘stage manuscripts  
- Ensuring readiness before sending to journals or conferences  

---

# ðŸŽ¯ Purpose of This Repository

These three prompts provide **structured, reproducible tools** for:

- Testing the strength and clarity of arguments  
- Validating whether technical documents are ready for public release  
- Evaluating whether academic papers meet baseline submission quality  

They deliver **processâ€‘based structure**, not subjective judgment, ensuring consistent evaluation regardless of which LLM is used.

---

# ðŸ§  How to Use These Prompts

1. Open the desired prompt file using the links above.  
2. Paste the full prompt into your preferred LLM.  
3. Provide the argument, report, or paper when requested.  
4. Allow the model to follow the structured evaluation steps.  
5. Review the output as a formal assessment.  

Optional: run the same prompt across multiple LLMs to compare reasoning quality.

---

# ðŸ“„ License

This project is licensed under the  
**Creative Commons Attribution 4.0 International (CC BY 4.0)** license.

You are free to:

- **Share** â€” copy and redistribute  
- **Adapt** â€” remix, transform, and build upon  

With proper attribution.

License file:  
[`LICENSE`](LICENSE)

---

# ðŸ“‚ Direct File Links

- **Structured Logical Argument Test Prompt**  
  https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Structured_Logical_Argument_Test_Prompt.md  

- **Technical Report / White Paper Review Prompt**  
  https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Technical_Report_White_Paper_Pre_Publication_Test_Prompt.md  

- **Academic Paper Publicationâ€‘Readiness Evaluation Prompt**  
  https://github.com/russnida-repo/LMM-Prompts/blob/main/Prompts/Academic%20Paper%20Review%20Prompt.md  

---

If you'd like, I can also:
- Add GitHub badges  
- Create a GitHub Pages landing page  
- Add versioning & changelog  
- Provide usage examples  
- Build a Quickâ€‘Start Guide for all three prompts  
